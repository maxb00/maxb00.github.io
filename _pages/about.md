---
layout: about
title: about
permalink: /
subtitle: Software Engineer, Research Assistant, and aspiring Scientist.

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>DW22-284</p>
    <p>11 Gore Rd.</p>
    <p>Webster, MA 01570</p>

news: false  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---

I am an aspiring Research Scientist searching for a lab and insitution for a research or teching assistanship and to pursure a PhD beginning Fall '25. I am currently working as a Software Engineer for MAPFRE Insurance, modifying and configuring the insurance policy management software Guidewire using Gosu (a JVM language). My main research interests lie in Natural Language Processing, Ethical AI, and model interpretability.   


I have done extensive work with pre-trained Transformers such as OpenAI's GPT-3.5, Meta's OPT, and Google's T5, investigating vector representations of tokens called "embeddings" for applications in transfer learning, fine tuning, and interdisciplinary investigation. Moving forward, I am interested programmatically investigating and characterizing "truth" in embeddings and other artifacts of language generation to further our understandings of how Large Language Models generate output and "hallucinate". Principally, investigating the direction (or lack thereof) of truth in Large Language Models. On a broader scale, I am determined to investigate how our efforts towards AI alignment (and fine tuning in general) modifies the hyperdimensional "plane of best fit" modeled by the Transformer. For example, I am curious how the same vector embedding may change in use within a model while that model undergoes different kinds of fine-tuning. Particularly when we fine-tune to remove direct utterances of predjudiced ideas, do algorithms simply trim problematic outputs, redistribute explicit harm into other less explicit outputs, or somethign else entirely? And how does the model itself reallocate the space previously used to hold predjudices language and ideals?  