---
layout: about
title: about
permalink: /
subtitle: Software Engineer, Research Assistant, and aspiring Scientist.

profile:
  align: right
  image: prof_pic.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>DW22-284</p>
    <p>11 Gore Rd.</p>
    <p>Webster, MA 01570</p>

news: false  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: true # includes a list of papers marked as "selected={true}"
social: true  # includes social icons at the bottom of the page
---
### Introducion
I am an aspiring Research Scientist searching for a lab to complete an assistantship and to pursure a PhD with beginning in Fall '25. I am currently working as a Software Engineer for MAPFRE Insurance, modifying and configuring the insurance policy management software Guidewire using Gosu (a JVM language). My main research interests lie in Natural Language Processing, Ethical AI, and model interpretability.   

### Research
I have done extensive work with pre-trained Transformers such as OpenAI's GPT-3.5, Meta's OPT, and Google's T5, investigating vector representations of tokens called "embeddings" for applications in transfer learning, fine tuning, and interdisciplinary investigation. Moving forward, I am interested programmatically investigating and characterizing "truth" in embeddings and other artifacts of artificial language generation to further our understandings of how Large Language Models generate output and can come to "hallucinate". Principally, I would be investigating the direction (or lack thereof) of truth in embeddings from Large Language Models. On a broader scale, I am determined to investigate how our efforts towards AI alignment (and fine tuning in general) modifies the hyperdimensional "plane of best fit" modeled by the Transformer. For example, I am curious how the same vector embedding may change in use within a model while that model undergoes different kinds of fine-tuning. Particularly when we fine-tune to remove direct utterances of predjudiced ideas, do algorithms simply trim problematic outputs, redistribute explicit harm into other less explicit outputs, or something else entirely? And how does the model itself reallocate the space previously used to hold predjudices language and ideals?  

### Other Interests
I have spent past lives as a chamber choir singer, as well as a songwriter and guitarist. I got my start as a teen with District and State choirs in Massachusetts, and kindled a love for live performance at open mic's with a ukulele in the mid 2010's. I love live music and the occasional craft beer, so catch me at a local brewery for community events and local bands! Music is a continued interest of mine, and I hope to bring it with me wherever I go.